{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from utils import EstimatorSelectionHelper\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "data = pd.read_csv(\"dataset.csv\", header=0)\n",
    "data.head()\n",
    "\n",
    "\n",
    "y = data['ReachedCompositeEndpoint']\n",
    "X = data[['Sex', 'MontrealLocationDiagnosis', 'HBI', 'TimeFromLastUpdateMontrealMonths', 'MaxMontrealBehaviour', 'Plt', 'AbdominalPainId', 'LiquidStoolsPerDay', 'MouthUlcers', 'qryAverageCalprosBeforeCompositeEndpoint.CountOfNumericalCalpro', 'qryAverageCalprosBeforeCompositeEndpoint.MinOfNumericalCalpro', 'qryAverageCalprosBeforeCompositeEndpoint.AvgOfNumericalCalpro', 'qryAverageCalprosBeforeCompositeEndpoint.AvgOfLogCalprotectin', 'qryAverageCalprosBeforeCompositeEndpoint.MaxOfNumericalCalpro', 'qryAverageCalprosBeforeMontrealIncrease.CountOfNumericalCalpro',\n",
    "          'qryAverageCalprosBeforeMontrealIncrease.MaxOfNumericalCalpro', 'qryAverageCalprosBeforeMontrealIncrease.AvgOfNumericalCalpro']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=0),\n",
    "    'MLPClassifier': MLPClassifier(max_iter=300, random_state=0),\n",
    "    \"SVC\": SVC(random_state=0, probability=True)\n",
    "     \n",
    "\n",
    "}\n",
    "old_models = {\n",
    "    #'MLPClassifier': MLPClassifier(max_iter=300, random_state=0),\n",
    "    \"SVC\": SVC(random_state=0)\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'LogisticRegression': {'penalty': ['l1', 'l2'],\n",
    "                           'C': np.logspace(-4, 4, 20)},\n",
    "    'RandomForestClassifier': {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], \"max_depth\": [1,3,5,6], \"min_samples_split\": [1.0,2,3,4]},\n",
    "    \"MLPClassifier\": {\n",
    "        'hidden_layer_sizes': [(100,), (100, 75, 50), (100, 50, 4), (100, 50, 25)],\n",
    "        'activation': ['logistic'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.00001, 0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "\n",
    "    },\n",
    "    'SVC': {'C': [0.1, 1], 'gamma': [1, 0.1], 'kernel': [\n",
    "         \"linear\"]},\n",
    "    'GaussianNB': {'var_smoothing': [0.1,0.5,1,2,3,0.00001]},\n",
    "    'KNN': {'leaf_size' : list(range(1,50)),\n",
    "'n_neighbors' : list(range(1,30)),\n",
    "'p' : [1,2]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:   26.0s finished\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "Fitting 10 folds for each of 440 candidates, totalling 4400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  77 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=2)]: Done 227 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=2)]: Done 477 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=2)]: Done 827 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=2)]: Done 1277 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=2)]: Done 1827 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=2)]: Done 2477 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=2)]: Done 3227 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=2)]: Done 4077 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=2)]: Done 4400 out of 4400 | elapsed: 26.6min finished\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for MLPClassifier.\n",
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   28.3s\n"
     ]
    }
   ],
   "source": [
    "helper = EstimatorSelectionHelper(models, params)\n",
    "helper.fit(X_train, y_train, n_jobs=2, scoring='accuracy', refit=\"True\")\n",
    "means = helper.score_summary(sort_by='mean_score')\n",
    "maxs = helper.score_summary(sort_by='max_score').iloc[1:20]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=4.281332398719396, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "LogisticRegression\n",
      "0.8285232383808097\n",
      "0.9008264462809917\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=13, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestClassifier\n",
      "0.794508995502249\n",
      "0.8760330578512396\n",
      "MLPClassifier(activation='logistic', alpha=0.5, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 75, 50), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=0, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "MLPClassifier\n",
      "0.75\n",
      "0.8801652892561983\n"
     ]
    }
   ],
   "source": [
    "for k in helper.grid_searches:\n",
    "    best_estimator = helper.grid_searches[k].best_estimator_\n",
    "    print(best_estimator)\n",
    "    test_roc = roc_auc_score(y_test, best_estimator.predict(X_test))\n",
    "    test_acc = accuracy_score(y_test, best_estimator.predict(X_test))\n",
    "    print(k)\n",
    "    print(test_roc)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>C</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.88476</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0414279</td>\n",
       "      <td>4.28133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>11.2884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>3792.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>1438.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>545.559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>206.914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>78.476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>29.7635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882943</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0390816</td>\n",
       "      <td>29.7635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>206.914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>78.476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>3792.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.881157</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0412688</td>\n",
       "      <td>4.28133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.881157</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0381692</td>\n",
       "      <td>1438.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879465</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0417383</td>\n",
       "      <td>1.62378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879465</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.043188</td>\n",
       "      <td>1.62378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879402</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0387104</td>\n",
       "      <td>545.559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879402</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0387104</td>\n",
       "      <td>11.2884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.877711</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0428873</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.87417</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0456742</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.870661</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0472998</td>\n",
       "      <td>0.233572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.865304</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0446517</td>\n",
       "      <td>0.233572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.861763</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0436069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.5</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.861763</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0436069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.5</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.86004</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0479623</td>\n",
       "      <td>0.0335982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.86004</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0479623</td>\n",
       "      <td>0.0127427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.860009</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.041804</td>\n",
       "      <td>0.0885867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.860009</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0473759</td>\n",
       "      <td>0.00483293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.733646</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.0115914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  estimator min_score mean_score max_score  std_score  \\\n",
       "23       LogisticRegression  0.807018    0.88476  0.964286  0.0414279   \n",
       "24       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "38       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "36       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "34       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "32       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "30       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "28       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "26       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "27       LogisticRegression  0.807018   0.882943  0.946429  0.0390816   \n",
       "31       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "29       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "39       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "37       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "22       LogisticRegression  0.807018   0.881157  0.947368  0.0412688   \n",
       "35       LogisticRegression  0.807018   0.881157  0.946429  0.0381692   \n",
       "21       LogisticRegression  0.807018   0.879465  0.964286  0.0417383   \n",
       "20       LogisticRegression  0.807018   0.879465  0.964286   0.043188   \n",
       "33       LogisticRegression  0.807018   0.879402  0.946429  0.0387104   \n",
       "25       LogisticRegression  0.807018   0.879402  0.946429  0.0387104   \n",
       "18       LogisticRegression  0.807018   0.877711  0.964286  0.0428873   \n",
       "19       LogisticRegression  0.789474    0.87417  0.964286  0.0456742   \n",
       "16       LogisticRegression  0.789474   0.870661  0.964286  0.0472998   \n",
       "17       LogisticRegression   0.77193   0.865304  0.946429  0.0446517   \n",
       "522           MLPClassifier  0.807018   0.861763  0.946429  0.0436069   \n",
       "523           MLPClassifier  0.807018   0.861763  0.946429  0.0436069   \n",
       "13       LogisticRegression  0.754386    0.86004  0.946429  0.0479623   \n",
       "11       LogisticRegression  0.754386    0.86004  0.946429  0.0479623   \n",
       "15       LogisticRegression   0.77193   0.860009  0.928571   0.041804   \n",
       "9        LogisticRegression  0.754386   0.860009  0.946429  0.0473759   \n",
       "..                      ...       ...        ...       ...        ...   \n",
       "57   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "50   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "51   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "52   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "53   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "54   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "55   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "56   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "64   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "58   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "70   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "78   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "77   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "76   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "75   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "74   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "73   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "72   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "71   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "69   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "59   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "68   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "67   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "66   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "65   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "63   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "62   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "61   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "60   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "79   RandomForestClassifier  0.701754   0.733646  0.745455  0.0115914   \n",
       "\n",
       "              C activation alpha hidden_layer_sizes learning_rate max_depth  \\\n",
       "23      4.28133        NaN   NaN                NaN           NaN       NaN   \n",
       "24      11.2884        NaN   NaN                NaN           NaN       NaN   \n",
       "38        10000        NaN   NaN                NaN           NaN       NaN   \n",
       "36      3792.69        NaN   NaN                NaN           NaN       NaN   \n",
       "34      1438.45        NaN   NaN                NaN           NaN       NaN   \n",
       "32      545.559        NaN   NaN                NaN           NaN       NaN   \n",
       "30      206.914        NaN   NaN                NaN           NaN       NaN   \n",
       "28       78.476        NaN   NaN                NaN           NaN       NaN   \n",
       "26      29.7635        NaN   NaN                NaN           NaN       NaN   \n",
       "27      29.7635        NaN   NaN                NaN           NaN       NaN   \n",
       "31      206.914        NaN   NaN                NaN           NaN       NaN   \n",
       "29       78.476        NaN   NaN                NaN           NaN       NaN   \n",
       "39        10000        NaN   NaN                NaN           NaN       NaN   \n",
       "37      3792.69        NaN   NaN                NaN           NaN       NaN   \n",
       "22      4.28133        NaN   NaN                NaN           NaN       NaN   \n",
       "35      1438.45        NaN   NaN                NaN           NaN       NaN   \n",
       "21      1.62378        NaN   NaN                NaN           NaN       NaN   \n",
       "20      1.62378        NaN   NaN                NaN           NaN       NaN   \n",
       "33      545.559        NaN   NaN                NaN           NaN       NaN   \n",
       "25      11.2884        NaN   NaN                NaN           NaN       NaN   \n",
       "18     0.615848        NaN   NaN                NaN           NaN       NaN   \n",
       "19     0.615848        NaN   NaN                NaN           NaN       NaN   \n",
       "16     0.233572        NaN   NaN                NaN           NaN       NaN   \n",
       "17     0.233572        NaN   NaN                NaN           NaN       NaN   \n",
       "522         NaN   logistic   0.5      (100, 75, 50)      constant       NaN   \n",
       "523         NaN   logistic   0.5      (100, 75, 50)      adaptive       NaN   \n",
       "13    0.0335982        NaN   NaN                NaN           NaN       NaN   \n",
       "11    0.0127427        NaN   NaN                NaN           NaN       NaN   \n",
       "15    0.0885867        NaN   NaN                NaN           NaN       NaN   \n",
       "9    0.00483293        NaN   NaN                NaN           NaN       NaN   \n",
       "..          ...        ...   ...                ...           ...       ...   \n",
       "57          NaN        NaN   NaN                NaN           NaN         1   \n",
       "50          NaN        NaN   NaN                NaN           NaN         1   \n",
       "51          NaN        NaN   NaN                NaN           NaN         1   \n",
       "52          NaN        NaN   NaN                NaN           NaN         1   \n",
       "53          NaN        NaN   NaN                NaN           NaN         1   \n",
       "54          NaN        NaN   NaN                NaN           NaN         1   \n",
       "55          NaN        NaN   NaN                NaN           NaN         1   \n",
       "56          NaN        NaN   NaN                NaN           NaN         1   \n",
       "64          NaN        NaN   NaN                NaN           NaN         1   \n",
       "58          NaN        NaN   NaN                NaN           NaN         1   \n",
       "70          NaN        NaN   NaN                NaN           NaN         1   \n",
       "78          NaN        NaN   NaN                NaN           NaN         1   \n",
       "77          NaN        NaN   NaN                NaN           NaN         1   \n",
       "76          NaN        NaN   NaN                NaN           NaN         1   \n",
       "75          NaN        NaN   NaN                NaN           NaN         1   \n",
       "74          NaN        NaN   NaN                NaN           NaN         1   \n",
       "73          NaN        NaN   NaN                NaN           NaN         1   \n",
       "72          NaN        NaN   NaN                NaN           NaN         1   \n",
       "71          NaN        NaN   NaN                NaN           NaN         1   \n",
       "69          NaN        NaN   NaN                NaN           NaN         1   \n",
       "59          NaN        NaN   NaN                NaN           NaN         1   \n",
       "68          NaN        NaN   NaN                NaN           NaN         1   \n",
       "67          NaN        NaN   NaN                NaN           NaN         1   \n",
       "66          NaN        NaN   NaN                NaN           NaN         1   \n",
       "65          NaN        NaN   NaN                NaN           NaN         1   \n",
       "63          NaN        NaN   NaN                NaN           NaN         1   \n",
       "62          NaN        NaN   NaN                NaN           NaN         1   \n",
       "61          NaN        NaN   NaN                NaN           NaN         1   \n",
       "60          NaN        NaN   NaN                NaN           NaN         1   \n",
       "79          NaN        NaN   NaN                NaN           NaN         1   \n",
       "\n",
       "    min_samples_split n_estimators penalty solver  \n",
       "23                NaN          NaN      l2    NaN  \n",
       "24                NaN          NaN      l1    NaN  \n",
       "38                NaN          NaN      l1    NaN  \n",
       "36                NaN          NaN      l1    NaN  \n",
       "34                NaN          NaN      l1    NaN  \n",
       "32                NaN          NaN      l1    NaN  \n",
       "30                NaN          NaN      l1    NaN  \n",
       "28                NaN          NaN      l1    NaN  \n",
       "26                NaN          NaN      l1    NaN  \n",
       "27                NaN          NaN      l2    NaN  \n",
       "31                NaN          NaN      l2    NaN  \n",
       "29                NaN          NaN      l2    NaN  \n",
       "39                NaN          NaN      l2    NaN  \n",
       "37                NaN          NaN      l2    NaN  \n",
       "22                NaN          NaN      l1    NaN  \n",
       "35                NaN          NaN      l2    NaN  \n",
       "21                NaN          NaN      l2    NaN  \n",
       "20                NaN          NaN      l1    NaN  \n",
       "33                NaN          NaN      l2    NaN  \n",
       "25                NaN          NaN      l2    NaN  \n",
       "18                NaN          NaN      l1    NaN  \n",
       "19                NaN          NaN      l2    NaN  \n",
       "16                NaN          NaN      l1    NaN  \n",
       "17                NaN          NaN      l2    NaN  \n",
       "522               NaN          NaN     NaN   adam  \n",
       "523               NaN          NaN     NaN   adam  \n",
       "13                NaN          NaN      l2    NaN  \n",
       "11                NaN          NaN      l2    NaN  \n",
       "15                NaN          NaN      l2    NaN  \n",
       "9                 NaN          NaN      l2    NaN  \n",
       "..                ...          ...     ...    ...  \n",
       "57                  2          800     NaN    NaN  \n",
       "50                  2          100     NaN    NaN  \n",
       "51                  2          200     NaN    NaN  \n",
       "52                  2          300     NaN    NaN  \n",
       "53                  2          400     NaN    NaN  \n",
       "54                  2          500     NaN    NaN  \n",
       "55                  2          600     NaN    NaN  \n",
       "56                  2          700     NaN    NaN  \n",
       "64                  3          500     NaN    NaN  \n",
       "58                  2          900     NaN    NaN  \n",
       "70                  4          100     NaN    NaN  \n",
       "78                  4          900     NaN    NaN  \n",
       "77                  4          800     NaN    NaN  \n",
       "76                  4          700     NaN    NaN  \n",
       "75                  4          600     NaN    NaN  \n",
       "74                  4          500     NaN    NaN  \n",
       "73                  4          400     NaN    NaN  \n",
       "72                  4          300     NaN    NaN  \n",
       "71                  4          200     NaN    NaN  \n",
       "69                  3         1000     NaN    NaN  \n",
       "59                  2         1000     NaN    NaN  \n",
       "68                  3          900     NaN    NaN  \n",
       "67                  3          800     NaN    NaN  \n",
       "66                  3          700     NaN    NaN  \n",
       "65                  3          600     NaN    NaN  \n",
       "63                  3          400     NaN    NaN  \n",
       "62                  3          300     NaN    NaN  \n",
       "61                  3          200     NaN    NaN  \n",
       "60                  3          100     NaN    NaN  \n",
       "79                  4         1000     NaN    NaN  \n",
       "\n",
       "[528 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=4.281332398719396, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=13, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "MLPClassifier(activation='logistic', alpha=0.5, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 75, 50), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=0, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "for k in helper.grid_searches:\n",
    "    best_estimator = helper.grid_searches[k].best_estimator_\n",
    "    print(best_estimator)\n",
    "    fpr, tpr, tres  = roc_curve(y_test, best_estimator.predict_proba(X_test)[:,1], pos_label=0)\n",
    "    plt.plot(fpr,tpr, label=k)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "helper.grid_searches[0].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.grid_searches[\"MLPClassifier\"].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
