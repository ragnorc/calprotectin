{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from utils import EstimatorSelectionHelper\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "data = pd.read_csv(\"dataset.csv\", header=0)\n",
    "data.head()\n",
    "\n",
    "\n",
    "y = data['ReachedCompositeEndpoint']\n",
    "X = data[['Sex', 'MontrealLocationDiagnosis', 'HBI', 'TimeFromLastUpdateMontrealMonths', 'MaxMontrealBehaviour', 'Plt', 'AbdominalPainId', 'LiquidStoolsPerDay', 'MouthUlcers', 'qryAverageCalprosBeforeCompositeEndpoint.CountOfNumericalCalpro', 'qryAverageCalprosBeforeCompositeEndpoint.MinOfNumericalCalpro', 'qryAverageCalprosBeforeCompositeEndpoint.AvgOfNumericalCalpro', 'qryAverageCalprosBeforeCompositeEndpoint.AvgOfLogCalprotectin', 'qryAverageCalprosBeforeCompositeEndpoint.MaxOfNumericalCalpro', 'qryAverageCalprosBeforeMontrealIncrease.CountOfNumericalCalpro',\n",
    "          'qryAverageCalprosBeforeMontrealIncrease.MaxOfNumericalCalpro', 'qryAverageCalprosBeforeMontrealIncrease.AvgOfNumericalCalpro']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=0),\n",
    "    'MLPClassifier': MLPClassifier(max_iter=300, random_state=0),\n",
    "     \n",
    "\n",
    "}\n",
    "old_models = {\n",
    "    #'MLPClassifier': MLPClassifier(max_iter=300, random_state=0),\n",
    "    \"SVC\": SVC(random_state=0)\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'LogisticRegression': {'penalty': ['l1', 'l2'],\n",
    "                           'C': np.logspace(-4, 4, 20)},\n",
    "    'RandomForestClassifier': {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], \"max_depth\": [1,3,5,10,13,14,16,50,100, 200, 300], \"min_samples_split\": [1.0,2,3,4]},\n",
    "    \"MLPClassifier\": {\n",
    "        'hidden_layer_sizes': [(100,), (100, 75, 50), (100, 50, 4), (100, 50, 25)],\n",
    "        'activation': ['logistic'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.00001, 0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "\n",
    "    },\n",
    "    'SVC': {'C': [0.1, 1], 'gamma': [1, 0.1], 'kernel': [\n",
    "         \"linear\"]},\n",
    "    'GaussianNB': {'var_smoothing': [0.1,0.5,1,2,3,0.00001]},\n",
    "    'KNN': {'leaf_size' : list(range(1,50)),\n",
    "'n_neighbors' : list(range(1,30)),\n",
    "'p' : [1,2]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done 301 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=2)]: Done 397 out of 400 | elapsed:   25.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done 400 out of 400 | elapsed:   25.4s finished\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForestClassifier.\n",
      "Fitting 10 folds for each of 330 candidates, totalling 3300 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\", line 333, in fit\n    for i, t in enumerate(trees))\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 917, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 759, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 716, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 182, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 549, in __init__\n    self.results = batch()\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\", line 119, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/tree/tree.py\", line 801, in fit\n    X_idx_sorted=X_idx_sorted)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/tree/tree.py\", line 194, in fit\n    % self.min_samples_split)\nValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8d47fcece6c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimatorSelectionHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmaxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cv, n_jobs, verbose, scoring, refit)\u001b[0m\n\u001b[1;32m     24\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                               return_train_score=True)\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_searches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1"
     ]
    }
   ],
   "source": [
    "helper = EstimatorSelectionHelper(models, params)\n",
    "helper.fit(X_train, y_train, n_jobs=2, scoring='accuracy', refit=\"True\")\n",
    "means = helper.score_summary(sort_by='mean_score')\n",
    "maxs = helper.score_summary(sort_by='max_score').iloc[1:20]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=4.281332398719396, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "LogisticRegression\n",
      "0.8285232383808097\n",
      "0.9008264462809917\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "RandomForestClassifier\n",
      "0.7972263868065967\n",
      "0.8801652892561983\n",
      "MLPClassifier(activation='logistic', alpha=0.5, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 75, 50), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=0, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "MLPClassifier\n",
      "0.75\n",
      "0.8801652892561983\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False)\n",
      "SVC\n",
      "0.7931034482758621\n",
      "0.9008264462809917\n"
     ]
    }
   ],
   "source": [
    "for k in helper.grid_searches:\n",
    "    best_estimator = helper.grid_searches[k].best_estimator_\n",
    "    print(best_estimator)\n",
    "    test_roc = roc_auc_score(y_test, best_estimator.predict(X_test))\n",
    "    test_acc = accuracy_score(y_test, best_estimator.predict(X_test))\n",
    "    print(k)\n",
    "    print(test_roc)\n",
    "    print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "      <th>C</th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>gamma</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>kernel</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.88476</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0414279</td>\n",
       "      <td>4.28133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>206.914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>545.559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>11.2884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>3792.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>29.7635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>1438.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0391778</td>\n",
       "      <td>78.476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882943</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0390816</td>\n",
       "      <td>29.7635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>206.914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>78.476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>3792.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.882911</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0383494</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.881157</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0412688</td>\n",
       "      <td>4.28133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.881157</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0381692</td>\n",
       "      <td>1438.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879465</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.043188</td>\n",
       "      <td>1.62378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879465</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0417383</td>\n",
       "      <td>1.62378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879402</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0387104</td>\n",
       "      <td>545.559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.879402</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0387104</td>\n",
       "      <td>11.2884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.877711</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0428873</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.87417</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0456742</td>\n",
       "      <td>0.615848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.872416</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.040752</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.872416</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.040752</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.870661</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.0472998</td>\n",
       "      <td>0.233572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.865304</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0446517</td>\n",
       "      <td>0.233572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.865272</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0371747</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.865272</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0371747</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.861763</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0436069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.861763</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0436069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.84212</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.0466964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 4)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.841991</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.0363473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.841991</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.0363473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.840396</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.0408298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.840396</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.0408298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.840302</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.0285708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.840302</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.0285708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.84027</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.0287369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.84027</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.0287369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.83654</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.0221995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.83654</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.0221995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.835005</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.0431986</td>\n",
       "      <td>0.00483293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.833191</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0255483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.833191</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0255483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 50, 25)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.82783</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.0443842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.82783</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.0443842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(100, 75, 50)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.824384</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.040959</td>\n",
       "      <td>0.00183298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0395354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.801389</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.0376695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.801389</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0378506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.801389</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0378506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.801389</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0353269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.801325</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.0363097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.799635</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0387579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.79788</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0460385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.79788</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0460385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.796126</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.0417833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.788762</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.0348965</td>\n",
       "      <td>0.000695193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.744236</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>0.0136036</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.740727</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.0329948</td>\n",
       "      <td>0.000263665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  estimator min_score mean_score max_score  std_score  \\\n",
       "23       LogisticRegression  0.807018    0.88476  0.964286  0.0414279   \n",
       "30       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "32       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "38       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "24       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "36       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "26       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "34       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "28       LogisticRegression  0.807018   0.884697  0.946429  0.0391778   \n",
       "27       LogisticRegression  0.807018   0.882943  0.946429  0.0390816   \n",
       "31       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "29       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "37       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "39       LogisticRegression  0.807018   0.882911  0.946429  0.0383494   \n",
       "22       LogisticRegression  0.807018   0.881157  0.947368  0.0412688   \n",
       "35       LogisticRegression  0.807018   0.881157  0.946429  0.0381692   \n",
       "20       LogisticRegression  0.807018   0.879465  0.964286   0.043188   \n",
       "21       LogisticRegression  0.807018   0.879465  0.964286  0.0417383   \n",
       "33       LogisticRegression  0.807018   0.879402  0.946429  0.0387104   \n",
       "25       LogisticRegression  0.807018   0.879402  0.946429  0.0387104   \n",
       "18       LogisticRegression  0.807018   0.877711  0.964286  0.0428873   \n",
       "19       LogisticRegression  0.789474    0.87417  0.964286  0.0456742   \n",
       "101                     SVC  0.807018   0.872416  0.946429   0.040752   \n",
       "100                     SVC  0.807018   0.872416  0.946429   0.040752   \n",
       "16       LogisticRegression  0.789474   0.870661  0.964286  0.0472998   \n",
       "17       LogisticRegression   0.77193   0.865304  0.946429  0.0446517   \n",
       "98                      SVC  0.807018   0.865272  0.946429  0.0371747   \n",
       "99                      SVC  0.807018   0.865272  0.946429  0.0371747   \n",
       "93            MLPClassifier  0.807018   0.861763  0.946429  0.0436069   \n",
       "92            MLPClassifier  0.807018   0.861763  0.946429  0.0436069   \n",
       "..                      ...       ...        ...       ...        ...   \n",
       "71            MLPClassifier  0.767857    0.84212  0.946429  0.0466964   \n",
       "64            MLPClassifier  0.789474   0.841991  0.892857  0.0363473   \n",
       "65            MLPClassifier  0.789474   0.841991  0.892857  0.0363473   \n",
       "68            MLPClassifier   0.77193   0.840396  0.892857  0.0408298   \n",
       "69            MLPClassifier   0.77193   0.840396  0.892857  0.0408298   \n",
       "53            MLPClassifier  0.789474   0.840302  0.894737  0.0285708   \n",
       "52            MLPClassifier  0.789474   0.840302  0.894737  0.0285708   \n",
       "77            MLPClassifier  0.789474    0.84027  0.894737  0.0287369   \n",
       "76            MLPClassifier  0.789474    0.84027  0.894737  0.0287369   \n",
       "56            MLPClassifier  0.807018    0.83654  0.894737  0.0221995   \n",
       "57            MLPClassifier  0.807018    0.83654  0.894737  0.0221995   \n",
       "8        LogisticRegression  0.754386   0.835005  0.910714  0.0431986   \n",
       "73            MLPClassifier  0.789474   0.833191  0.872727  0.0255483   \n",
       "72            MLPClassifier  0.789474   0.833191  0.872727  0.0255483   \n",
       "61            MLPClassifier  0.719298    0.82783  0.890909  0.0443842   \n",
       "60            MLPClassifier  0.719298    0.82783  0.890909  0.0443842   \n",
       "6        LogisticRegression  0.736842   0.824384  0.859649   0.040959   \n",
       "46   RandomForestClassifier  0.719298   0.804961  0.872727  0.0395354   \n",
       "41   RandomForestClassifier  0.719298   0.801389  0.854545  0.0376695   \n",
       "44   RandomForestClassifier  0.719298   0.801389  0.872727  0.0378506   \n",
       "43   RandomForestClassifier  0.719298   0.801389  0.872727  0.0378506   \n",
       "47   RandomForestClassifier  0.736842   0.801389  0.872727  0.0353269   \n",
       "42   RandomForestClassifier  0.719298   0.801325  0.854545  0.0363097   \n",
       "45   RandomForestClassifier  0.719298   0.799635  0.872727  0.0387579   \n",
       "49   RandomForestClassifier  0.684211    0.79788  0.872727  0.0460385   \n",
       "40   RandomForestClassifier  0.684211    0.79788  0.872727  0.0460385   \n",
       "48   RandomForestClassifier  0.701754   0.796126  0.872727  0.0417833   \n",
       "4        LogisticRegression  0.736842   0.788762  0.842105  0.0348965   \n",
       "0        LogisticRegression  0.732143   0.744236   0.77193  0.0136036   \n",
       "2        LogisticRegression  0.684211   0.740727  0.789474  0.0329948   \n",
       "\n",
       "               C activation  alpha gamma hidden_layer_sizes  kernel  \\\n",
       "23       4.28133        NaN    NaN   NaN                NaN     NaN   \n",
       "30       206.914        NaN    NaN   NaN                NaN     NaN   \n",
       "32       545.559        NaN    NaN   NaN                NaN     NaN   \n",
       "38         10000        NaN    NaN   NaN                NaN     NaN   \n",
       "24       11.2884        NaN    NaN   NaN                NaN     NaN   \n",
       "36       3792.69        NaN    NaN   NaN                NaN     NaN   \n",
       "26       29.7635        NaN    NaN   NaN                NaN     NaN   \n",
       "34       1438.45        NaN    NaN   NaN                NaN     NaN   \n",
       "28        78.476        NaN    NaN   NaN                NaN     NaN   \n",
       "27       29.7635        NaN    NaN   NaN                NaN     NaN   \n",
       "31       206.914        NaN    NaN   NaN                NaN     NaN   \n",
       "29        78.476        NaN    NaN   NaN                NaN     NaN   \n",
       "37       3792.69        NaN    NaN   NaN                NaN     NaN   \n",
       "39         10000        NaN    NaN   NaN                NaN     NaN   \n",
       "22       4.28133        NaN    NaN   NaN                NaN     NaN   \n",
       "35       1438.45        NaN    NaN   NaN                NaN     NaN   \n",
       "20       1.62378        NaN    NaN   NaN                NaN     NaN   \n",
       "21       1.62378        NaN    NaN   NaN                NaN     NaN   \n",
       "33       545.559        NaN    NaN   NaN                NaN     NaN   \n",
       "25       11.2884        NaN    NaN   NaN                NaN     NaN   \n",
       "18      0.615848        NaN    NaN   NaN                NaN     NaN   \n",
       "19      0.615848        NaN    NaN   NaN                NaN     NaN   \n",
       "101            1        NaN    NaN   0.1                NaN  linear   \n",
       "100            1        NaN    NaN     1                NaN  linear   \n",
       "16      0.233572        NaN    NaN   NaN                NaN     NaN   \n",
       "17      0.233572        NaN    NaN   NaN                NaN     NaN   \n",
       "98           0.1        NaN    NaN     1                NaN  linear   \n",
       "99           0.1        NaN    NaN   0.1                NaN  linear   \n",
       "93           NaN   logistic    0.5   NaN      (100, 75, 50)     NaN   \n",
       "92           NaN   logistic    0.5   NaN      (100, 75, 50)     NaN   \n",
       "..           ...        ...    ...   ...                ...     ...   \n",
       "71           NaN   logistic   0.01   NaN       (100, 50, 4)     NaN   \n",
       "64           NaN   logistic  0.001   NaN      (100, 50, 25)     NaN   \n",
       "65           NaN   logistic  0.001   NaN      (100, 50, 25)     NaN   \n",
       "68           NaN   logistic   0.01   NaN      (100, 75, 50)     NaN   \n",
       "69           NaN   logistic   0.01   NaN      (100, 75, 50)     NaN   \n",
       "53           NaN   logistic  1e-05   NaN      (100, 75, 50)     NaN   \n",
       "52           NaN   logistic  1e-05   NaN      (100, 75, 50)     NaN   \n",
       "77           NaN   logistic   0.05   NaN      (100, 75, 50)     NaN   \n",
       "76           NaN   logistic   0.05   NaN      (100, 75, 50)     NaN   \n",
       "56           NaN   logistic  1e-05   NaN      (100, 50, 25)     NaN   \n",
       "57           NaN   logistic  1e-05   NaN      (100, 50, 25)     NaN   \n",
       "8     0.00483293        NaN    NaN   NaN                NaN     NaN   \n",
       "73           NaN   logistic   0.01   NaN      (100, 50, 25)     NaN   \n",
       "72           NaN   logistic   0.01   NaN      (100, 50, 25)     NaN   \n",
       "61           NaN   logistic  0.001   NaN      (100, 75, 50)     NaN   \n",
       "60           NaN   logistic  0.001   NaN      (100, 75, 50)     NaN   \n",
       "6     0.00183298        NaN    NaN   NaN                NaN     NaN   \n",
       "46           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "41           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "44           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "43           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "47           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "42           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "45           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "49           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "40           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "48           NaN        NaN    NaN   NaN                NaN     NaN   \n",
       "4    0.000695193        NaN    NaN   NaN                NaN     NaN   \n",
       "0         0.0001        NaN    NaN   NaN                NaN     NaN   \n",
       "2    0.000263665        NaN    NaN   NaN                NaN     NaN   \n",
       "\n",
       "    learning_rate n_estimators penalty solver  \n",
       "23            NaN          NaN      l2    NaN  \n",
       "30            NaN          NaN      l1    NaN  \n",
       "32            NaN          NaN      l1    NaN  \n",
       "38            NaN          NaN      l1    NaN  \n",
       "24            NaN          NaN      l1    NaN  \n",
       "36            NaN          NaN      l1    NaN  \n",
       "26            NaN          NaN      l1    NaN  \n",
       "34            NaN          NaN      l1    NaN  \n",
       "28            NaN          NaN      l1    NaN  \n",
       "27            NaN          NaN      l2    NaN  \n",
       "31            NaN          NaN      l2    NaN  \n",
       "29            NaN          NaN      l2    NaN  \n",
       "37            NaN          NaN      l2    NaN  \n",
       "39            NaN          NaN      l2    NaN  \n",
       "22            NaN          NaN      l1    NaN  \n",
       "35            NaN          NaN      l2    NaN  \n",
       "20            NaN          NaN      l1    NaN  \n",
       "21            NaN          NaN      l2    NaN  \n",
       "33            NaN          NaN      l2    NaN  \n",
       "25            NaN          NaN      l2    NaN  \n",
       "18            NaN          NaN      l1    NaN  \n",
       "19            NaN          NaN      l2    NaN  \n",
       "101           NaN          NaN     NaN    NaN  \n",
       "100           NaN          NaN     NaN    NaN  \n",
       "16            NaN          NaN      l1    NaN  \n",
       "17            NaN          NaN      l2    NaN  \n",
       "98            NaN          NaN     NaN    NaN  \n",
       "99            NaN          NaN     NaN    NaN  \n",
       "93       adaptive          NaN     NaN   adam  \n",
       "92       constant          NaN     NaN   adam  \n",
       "..            ...          ...     ...    ...  \n",
       "71       adaptive          NaN     NaN   adam  \n",
       "64       constant          NaN     NaN   adam  \n",
       "65       adaptive          NaN     NaN   adam  \n",
       "68       constant          NaN     NaN   adam  \n",
       "69       adaptive          NaN     NaN   adam  \n",
       "53       adaptive          NaN     NaN   adam  \n",
       "52       constant          NaN     NaN   adam  \n",
       "77       adaptive          NaN     NaN   adam  \n",
       "76       constant          NaN     NaN   adam  \n",
       "56       constant          NaN     NaN   adam  \n",
       "57       adaptive          NaN     NaN   adam  \n",
       "8             NaN          NaN      l1    NaN  \n",
       "73       adaptive          NaN     NaN   adam  \n",
       "72       constant          NaN     NaN   adam  \n",
       "61       adaptive          NaN     NaN   adam  \n",
       "60       constant          NaN     NaN   adam  \n",
       "6             NaN          NaN      l1    NaN  \n",
       "46            NaN          700     NaN    NaN  \n",
       "41            NaN          200     NaN    NaN  \n",
       "44            NaN          500     NaN    NaN  \n",
       "43            NaN          400     NaN    NaN  \n",
       "47            NaN          800     NaN    NaN  \n",
       "42            NaN          300     NaN    NaN  \n",
       "45            NaN          600     NaN    NaN  \n",
       "49            NaN         1000     NaN    NaN  \n",
       "40            NaN         1000     NaN    NaN  \n",
       "48            NaN          900     NaN    NaN  \n",
       "4             NaN          NaN      l1    NaN  \n",
       "0             NaN          NaN      l1    NaN  \n",
       "2             NaN          NaN      l1    NaN  \n",
       "\n",
       "[102 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=4.281332398719396, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=None,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "MLPClassifier(activation='logistic', alpha=0.5, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100, 75, 50), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=0, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1, kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "for k in helper.grid_searches:\n",
    "    best_estimator = helper.grid_searches[k].best_estimator_\n",
    "    print(best_estimator)\n",
    "    fpr, tpr, tres  = roc_curve(y_test, best_estimator.predict_proba(X_test)[:,1], pos_label=0)\n",
    "    plt.plot(fpr,tpr, label=k)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "helper.grid_searches[0].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.grid_searches[\"MLPClassifier\"].predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
